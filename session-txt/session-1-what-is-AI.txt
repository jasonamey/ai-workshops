SESSION 1 
What is AI?


________________


1)
Introduction 


AI is a transformational technology - and it’s also one of the most heated topics in public life right now. 


People encounter AI through wildly diverging claims: utopian promises on one side, existential warnings on the other. 


When a topic gets this heated, it’s not enough to just learn the tool. We need a deep understanding of the many perspectives that shape opinions on this polarizing technology.  What experiences, incentives, and assumptions shape the way people talk about AI?


While AI is unmistakably a transformational technology, attitudes toward it in the United States can be mixed, characterized by some strong skepticism regarding its potential impact on society:
  

McClain, C., Kennedy, B., Gottfried, J., Anderson, M., & Pasquini, G. (2025, April 3). How the U.S. Public and AI Experts View Artificial Intelligence. Pew Research Center. https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/


  

Kennedy, B., Yam, E., Kikuchi, E., Pula, I., & Fuentes, J. (2025, September 17). How Americans View AI and Its Impact on People and Society. Pew Research Center. https://www.pewresearch.org/science/2025/09/17/how-americans-view-ai-and-its-impact-on-people-and-society/




________________


2)


These attitudes are perhaps a reflection of much of the maximalist, sensationalist treatment of AI from many prominent leaders in technology along with countless discussions on social media:
 
  

https://www.cnbc.com/2025/03/26/bill-gates-on-ai-humans-wont-be-needed-for-most-things.html


  

https://www.reddit.com/r/Libraries/comments/1lurgh3/how_do_you_use_ai_at_work/


Public conversations about AI are often shaped by extreme claims. Headlines forecasting the replacement of entire professions coexist with equally forceful rejections of the technology as unethical or exploitative. These statements circulate through social media stripped of nuance and context. As a result, AI is often encountered as an existential threat or overheated promise - inevitably inviting strong reactions from a public understandably wary of the technology.


Rather than responding to these claims at face value, we need to ask: what interests, experiences, incentives, and assumptions shape them? This is where the concept of context becomes essential.


________________


3)
Context and college education


A crucial component of college education is the mastery of context. This involves examining the diverse perspectives surrounding a question and evaluating whether a viewpoint is objectively 'correct' or if it is primarily an expression of the holder’s unique background.
AI is a great example of this kind of discussion. 
My background as a librarian at Baruch College represents a perfect illustration of context as I come to this workshop occupying three overlapping roles: citizen, instructor and librarian. 
Citizen:
  

	Instructor: 
  

	Librarian: 
  



	Images Generated by Google Gemini Nano Banana 
Depending on which role I am inhabiting, I may hold different - and sometimes conflicting - views about AI. 
In this workshop, you will see that understanding information itself - how it is organized, accessed, and made discoverable - is essential to understanding AI. From that perspective, I am speaking to you primarily as a librarian.
Librarians have long been responsible for helping people navigate complex information systems, evaluate sources, and understand how knowledge is produced and controlled. For that reason, I believe librarians are uniquely positioned to play a critical role in AI education.
It is important not to interpret my discussion of AI as a full-throated endorsement of the technology, nor to assume that my perspective as a librarian is identical to my views as an instructor or as a citizen living in New York City. 
These roles come with different responsibilities, values, and constraints - and it is normal for tensions to arise among them.
You may find yourselves negotiating similar tensions. Many people hold personal reservations about AI while also recognizing that it is increasingly embedded in academic work and the modern workforce. Balancing skepticism, ethical concerns, and professional expectations is becoming a common - and unavoidable - experience.
This workshop does not ask you to adopt a particular stance on AI. Instead, it aims to equip you with the questions, vocabulary, and context needed to engage with these tools critically and on your own terms.
In the end, you’ll find that understanding AI is also a problem of understanding information: how it’s organized, what is publicly available, and what is discoverable by automated systems. That is exactly what librarians specialize in - and why I believe librarians have a responsibility to help lead this conversation.


Discussing how to use this technology is not the same as giving it a 'seal of approval.' My views as a librarian are often in dialogue - or maybe even in conflict - with my views as a citizen. 


You might feel that same tug-of-war - recognizing the utility of AI while harboring doubts about its impact. This workshop won't tell you what to think; instead, it will help you navigate that tension so you can engage with the world more deliberately.


Potential discussion questions
* What feelings or assumptions did you have about AI (excitement, fear, skepticism, curiosity)?
* Where do you think those views came from, your own experience with AI?
* News coverage, social media, school, work, or conversations with others?
________________


4) 
What is the purpose of a college education? 


Traditionally, higher education has been understood as a space for obtaining a liberal arts education - one that emphasizes broad intellectual development and critical thinking, often developed through close reading, careful argumentation, and engagement with scholarship and literature.


From this perspective, college is not primarily meant to train students for specific jobs, but to provide foundational skills that remain valuable across many professions and vocational responsibilities. These skills - analyzing complex texts, evaluating evidence, constructing arguments - are seen as enduring, even as particular technologies and job requirements change.


At the same time, college has increasingly been viewed as a pathway to employment. Students, families, and employers often expect institutions to teach practical skills that translate directly into an immediate job offer upon graduation. 


In this view, education functions explicitly as job preparation, and courses are evaluated by how effectively they equip students with tools they can immediately apply in the workforce. 


This longstanding tension between these two purposes - intellectual formation versus job training - has become even more pronounced with the emergence of AI and questions surrounding its role in college education. 


On one hand, “AI literacy” can be framed as a necessary workplace skill, much like proficiency with spreadsheets, databases, or programming languages. From this angle, teaching students how to use AI tools responsibly and effectively is simply an extension of career preparation. 


On the other hand, AI raises questions about whether its use aligns with the goals of liberal arts education. If AI automates tasks such as summarizing readings and conducting analysis, it may bypass the very cognitive tasks through which students develop necessary critical thinking skills.
________________


5)
This raises a central question for higher education: 


Can AI be integrated in ways that support a traditional liberal arts education? 


Some argue that AI, when used thoughtfully, can enhance learning by helping students explore ideas and reflect on their reasoning. Others worry that its widespread use encourages shortcuts and ultimately promotes plagiarism. 


At the same time, AI may be introducing new forms of literacy that don’t fit neatly into older distinctions between “skill training” and “intellectual development.” Effective engagement with AI increasingly requires understanding how training data influences results, how probabilistic systems generate responses, and why authoritative language can still produce misleading content. These competencies involve interpretation, skepticism, and critical evaluation - skills that align closely with the liberal arts tradition.
From this perspective, a larger set of questions unfolds:
* Will AI reshape how thinking and understanding are practiced?
* What does it mean to read, write, and evaluate knowledge in an AI-saturated environment?
* Should AI literacy be treated as a technical skill - or as a critical literacy?
* Should these skills be taught explicitly in college?


________________


6)
Baruch Policy on AI 


Baruch College’s policies on AI reflect the reality that there is no single, universally agreed-upon stance on the technology. 


While the College recognizes the growing importance of AI across teaching and research, it emphasizes transparency and thoughtful integration. This approach acknowledges a range of perspectives on how AI can be incorporated into teaching and learning, allowing room for disciplinary judgment and pedagogical discretion.


The use of AI in coursework is determined by the course instructor, as outlined in Baruch College’s Sample AI Use Policies for Course Syllabi.


These range of approaches towards AI include:


* Strictly Prohibited
* Limited Use for Editing
* Limited Use for Awareness
* AI Awareness with Critical Evaluation
* Guided Use with Attribution
* Integrated Use for Application,
* Co-created Policies with Students.


The use of AI in your coursework is contingent on the guidelines set by your course instructor - unauthorized use may be considered a violation of academic integrity.


This flexibility acknowledges that different disciplines and instructors may reasonably reach different conclusions about AI’s role in the classroom. Understanding and respecting these contextual differences is an essential part of navigating AI responsibly in an academic environment.


Discussion question


If you were teaching a course at Baruch College, what AI policy would you adopt in your syllabus?


________________


7)
What is “AI”? 


Understanding a given technology often begins with clarifying terms that seem familiar but carry different meanings in different contexts.


For instance: What is “data”?


At its simplest, data refers to recorded information - observations or measurements that can be stored and analyzed. In practice, however, the term is used in many different ways, and its meaning shifts depending on (again) context. 


Depending on how it is used, “data” can refer to:


* Data as files or tables: organized information stored in spreadsheets, databases, or datasets used for analysis.


* Data as personal information: details about individuals, such as names, locations, browsing behavior, or biometric identifiers.


* Data as institutional records: grades, financial aid information, medical records, or employment files maintained by organizations

* Data as a commercial asset: information collected, owned, licensed, or sold by companies.

* Data as usage allotment: your “data plan” - the monthly amount of cellular data you can use before speeds are reduced or fees apply.


And many others!

Each meaning carries different assumptions about ownership, privacy, accuracy, and ethics etc...


________________


8)
This same linguistic challenge applies to “AI.”


AI is often treated as a single technology, when it actually refers to many different systems and practices. Meaning shifts depending on who is speaking and why. Understanding AI, like understanding “data,” requires attention to context - what kind of system is involved, how it works, and what purposes it serves. 


You will often see AI introduced as this collection of nested circles:


  

https://commons.wikimedia.org/wiki/File:Unraveling_AI_Complexity_-_A_Comparative_View_of_AI,_Machine_Learning,_Deep_Learning,_and_Generative_AI.png


This visual is designed to show that 'Artificial Intelligence' isn't just one single technology, but a broad field with layers of specialization. 


As we move from the outer circle to the inner one, the technology becomes more specific.


________________


9)
Artificial Intelligence


At its broadest level, “Artificial Intelligence” refers to any approach that enables computers to perform tasks that would normally require human intelligence. This includes recognizing patterns, making decisions, solving problems, interpreting language, and adapting behavior based on experience or input. Importantly, AI is not a single technology or method, but an umbrella term that encompasses many different techniques developed over decades.


Understanding A requires recognizing that it spans a wide range of approaches with very different assumptions, capabilities, and limitations. 


________________


10)
Machine Learning


text about these examples


  



________________


11)
Deep Learning and Neural Networks
Text about these examples
  

Deep learning emerges as a response to the limitations of traditional machine learning when problems become extremely complex. At this scale, the challenge is no longer just selecting variables and fitting a model, but learning how information should be represented in the first place.
________________


Examples of the jump between these two : 
12)


For decades, computing technology has been adept at solving models of relatively low mathematical complexity. A perfect example is this simple 2D model for predicting academic performance.


In this scenario, we are working within the realm of traditional statistics, where we attempt to predict a single output (GPA) from just one variable (Hours Studied):


  

As shown in the illustration, the relationship between these data points follows a clear, predictable path that can be "solved" using basic algebra.
Earlier computers excelled at calculating the "best fit" line through these points. Because the parameters are few, a human can easily comprehend the logic behind the model’s conclusion: "As hours studied increase, the GPA increases at a consistent rate."
This is a "transparent" model. If the computer predicts a GPA of 3.5, we can look at the line on the graph and see exactly why it reached that conclusion.
________________


13)
But while this two-dimensional chart is a transparent model - one where the relationship between effort and outcome is easy to see - real-world problems rarely depend on a single variable.
For instance, in our academic prediction, suppose we were interested in a model that not only learned patterns from “hours studied,” but also from variables such as: prior subject knowledge, sleep quality, attendance, resource accessibility, class difficulty, student-to-teacher ratios, socioeconomic factors, test anxiety levels, extracurricular commitments, and even nutritional habits (etc…)?
If we extend our approach to include many, many features, this traditional approach to modeling academic success begins to break down. When the number of inputs grows into the thousands, or even millions (or beyond!), more powerful techniques become necessary.
It is this level of complexity - where meaning emerges from the interaction of vast numbers of signals - that is required to model phenomena such as language, speech, and other forms of behavior we typically associate with intelligence.


This shift can be visualized by contrasting two very different modeling approaches. Again, in the earlier academic performance example, a simple linear model attempts to explain outcomes using a small number of variables  - like hours studied  - and a single, transparent relationship. The logic of the model is easy for a human to follow: we can literally see the line and understand how study time translates into a GPA prediction.




  

In contrast, deep learning models are represented as layered networks of interconnected units, where each layer transforms the input data in increasingly abstract ways. 
In the classic neural network illustration, raw inputs enter on the left, pass through multiple hidden layers, and produce outputs on the right. Each layer recombines information from the previous one, enabling the model to capture interactions and patterns that would be impossible for a human to specify explicitly.
  

In a traditional model, we might manually input known variables like prior subject knowledge, sleep quality, attendance, resource accessibility, class difficulty, student-to-teacher ratios, socioeconomic factors, test anxiety levels, extracurricular commitments, and nutritional habits. However, human experts are often incapable of realizing the sheer number of "latent" or invisible parameters that actually drive student success.
Deep learning systems do not need to be told in advance which features matter. Through exposure to massive amounts of data, these models find relationships and signals that are too subtle or non-linear for human perception - essentially discovering "hidden" variables that describe the learning process more accurately than any human-authored list ever could.
This layered architecture allows deep learning systems to model phenomena that resist simple explanations. Language, speech, vision, and the complex cognitive behaviors we associate with intelligence depend on these subtle, non-linear relationships rather than direct cause-and-effect rules. By coordinating millions - or even billions - of parameters across multiple layers, these models can approximate such relationships at scale, trading literal interpretability for massive expressive power and flexibility.
Modern language models illustrate the scale of this shift: GPT-3 contains ~175 billion parameters. This is a dramatic contrast to the single-variable "line" model used to predict GPA based on study hours alone.


________________


14)
Generative AI
Generative AI refers to systems designed not just to analyze or predict outcomes, but to produce new content, such as text, images, audio, or code. Rather than outputting a single answer or classification, these models generate new material based on patterns learned from large collections of existing data. That is: computer generated documents. 
  

This capability builds on the layered architectures described earlier. By learning statistical patterns in language, images, or sound, generative models can produce outputs that resemble human-created work. Importantly, they do not retrieve stored answers or “understand” meaning; they generate responses by predicting what is likely to come next in a given context.
This makes generative AI both powerful and risky. Its outputs can appear authoritative - even when they are inaccurate or misleading. Used thoughtfully however, these systems can potentially support tasks such as drafting, summarization, translation, and idea exploration - so long as humans remain responsible for evaluating and interpreting the results.
Understanding generative AI, requires attention to context: how the system was trained, what data it relies on, and what it is designed to do.
In this workshop, we will focus specifically on generative AI tools that produce written language - systems designed to generate text-based outputs such as paragraphs, emails, summaries and full documents. These text-generating tools are among the most widely used forms of generative AI in academic life, and they raise especially important questions about reliability, scholarly authority, citation practices, and responsible use.


________________


15)
Examples of Gen AI 
________________


16)
AI as a “Prediction Machine” 
While AI can undoubtedly appear as a technological marvel - sometimes even seeming to mirror aspects of human intelligence - it is best understood as a “prediction machine.” 
AI’s outputs are not the result of understanding, or reasoning in a human sense, but of statistical prediction based on patterns learned from previously encountered data. What an AI system can produce is therefore constrained by what it has seen before.
The process of building this predictive capacity is known as training. During training, the system is exposed to large amounts of data and learns to estimate the likelihood of different outcomes given a particular input. Whether classifying images, generating text, or making recommendations, AI is continually predicting what is most likely to come next based on prior examples, rather than drawing on comprehension or lived experience.
Here is a prediction machine that can identify a “dog” or a “cat”: 
  

This image shows a simple example of AI as a prediction machine. The system at the center has been trained only on images labeled “dog” and “cat.” When it receives a new image, its task is to predict which of those two labels is most likely, based on patterns it learned during training.
Because the system has seen only images of dogs and cats, it is fundamentally limited to those categories. It does not know what a dog or cat is in a conceptual sense - it predicts dog or cat because similar visual patterns have been associated with those labels in the past.


________________


17)
If we were to show this system an image of a pig, it would still be forced to make a prediction. Since pig is not an available option, the system would incorrectly label the image as either “dog” or “cat,” choosing whichever seems statistically closest based on its training. 
This kind of error helps explain what is often called a “hallucination” in AI systems: the model confidently produces an answer even when it lacks the knowledge or categories needed to respond accurately.
The key point is that the AI is not “making things up” intentionally - it is doing exactly what it was designed to do: generate the most likely prediction given the data it has seen. Its mistakes reveal the boundaries of its training, not independent reasoning or understanding.


________________


18)
Here now is a prediction machine that can also identify  “pigs” and “horses”:
  





This image extends the earlier example by expanding what the prediction machine has been trained to recognize. Instead of learning from only dogs and cats, the system has now been trained on images labeled “dog,” “cat,” “pig,” and “horse.” As a result, when it encounters a new image, it can distribute its predictions across a larger set of possible categories.


The important point is that the AI’s capabilities did not change because it “understands” animals better, but because its training data changed. By seeing examples of pigs and horses during training, the system learned additional visual patterns associated with those labels. This allows it to make more accurate predictions within the boundaries of what it has seen before.


This illustrates a key principle of AI systems: their world is defined by their training data. 


Adding more categories increases their expressive range, but does not grant general knowledge or awareness. What looks like intelligence is, at every step, the result of constrained prediction within a predefined space of possibilities.


These limits hint at how understanding AI represents a significant information challenge - one that depends on knowing the types of data and information that is collected, organized and ultimately trained on by AI systems. 


This is precisely the kind of expertise librarians can bring to any discussion of AI.


________________


19)
AI systems as prediction machines now appear across a wide range of domains. 


Prediction-based models power computer vision systems that identify objects in images, recommendation engines that shape what we watch and buy, fraud detection systems in finance, navigation and perception systems in self-driving cars, and medical tools that assist in diagnosis and imaging.


Each of these applications raises important technical and ethical questions. However, this workshop will not attempt to survey AI in all of its forms. Instead, it largely focuses on the uses of AI that most directly affect academic life.


In particular, we will concentrate on AI systems that operate on language - systems that read, summarize, generate, and respond to text. These systems increasingly shape how students and researchers write, study, and evaluate information, making them especially consequential in academic settings.


________________


20)
Predicting words and language 


There is an episode of animated television show The Simpsons in which the town’s power plant owner (and the show’s stand-in for wealth and power) Mr. Burns gives Homer Simpson a tour of his extravagant mansion. 


As a display of excess and power, Mr. Burns proudly reveals a room full of monkeys furiously typing on typewriters, explaining that they are attempting to: “write the greatest novel known to man!”


Mr. Burns checks the output from one of the monkeys and reads aloud what is almost the famous opening line from Charles Dickens’ A Tale of Two Cities - only to discover a mangled version: “It was the best of times, it was the blurst of times.”


  



The joke lands because we recognize how close the monkey came to a meaningful sentence -  while still missing the mark in a way that’s both absurd and revealing. 
It’s a perfect metaphor for what happens when language is produced through sheer pattern and repetition: sometimes you get something that looks like real writing, but without the underlying intention, context, or understanding that makes it correct.
In a unique way, AI language tools aren’t as far from this characterization… with one crucial difference. 
Like the monkeys, language models do not understand meaning, intention, or creativity in a human sense. They are not “trying” to say anything. They are simply producing sequences of symbols.
A machine left to generate symbols at random - like a monkey at a typewriter - is overwhelmingly likely to produce meaningless incoherence like: “it was the blurst of times.” 
However what separates a language model from that room of monkeys is not creativity or insight, but constraint. At every step, the model is guided by probabilistic prediction, continually estimating which word is most likely to come next given everything that has come before. This predictive structure keeps the output coherent and grammatically correct, even though no understanding or intention is involved.  At every step, probability keeps the system on track - preventing it from wandering into gibberish.
In other words, AI doesn’t replace Mr. Burns’ monkeys with intelligence - it replaces randomness with statistical discipline. The result can look remarkably coherent, even thoughtful - but it is still the product of prediction, not understanding.
________________


21)
From prediction machines to Large Language Models


Just as the earlier prediction machines learned to assign labels like dog, cat, pig, or horse, language-based AI systems are trained to predict words. Instead of receiving images as input, these systems receive text. Instead of predicting an animal label, they predict what word, phrase, or sentence is most likely to come next, given the context so far. 


From this perspective, generating a paragraph, summarizing an article, or answering a question is not categorically different from classifying animals. In each case, the system is performing constrained prediction within a space defined by its training data.


AI now touches almost every corner of professional and social life: recommendation algorithms shape what we watch and buy, fraud detection models monitor financial transactions, facial recognition and surveillance systems scan public spaces, automation tools streamline customer service and scheduling; and predictive models influence hiring, healthcare, and even policing. 


But despite this wide range of AI applications, most public attention has narrowed onto one particular kind of AI: language tools that can talk back. Increasingly, when people say “AI,” what they really mean is a chatbot that generates fluent writing on demand.


These systems are called Large Language Models (LLMs). 


A Large Language Model is a deep learning model trained on vast amounts of text, designed to predict and generate language that looks human-written. LLMs have become so consequential on college campuses because so much of academic life is text-based: reading, writing, summarizing, brainstorming, revising, researching, explaining, and arguing. When students can generate polished paragraphs instantly, it changes how writing is produced, how knowledge is evaluated, and how learning is assessed. It also raises deeper questions about authority and trust - because these tools can sound confident even when they are incomplete, distorted, or wrong.


This brings us to the central transition into the next session: if LLMs are powerful language engines - but also fundamentally limited by prediction and training data - what does that mean for research and scholarship?


In the next workshop, we’ll attempt to demystify the LLM and begin a discussion on the academic implications of this significant technology.




This review covers your Session 1: What is AI? content. Your transition from musical "pitch and rhythm" to digital "Auto-Tune and quantization" is a brilliant pedagogical hook. It visualizes "precision" as a human skill earned through labor, making the later discussion about AI as a "shortcut" much more impactful.
Below are suggestions for integrating your research articles into the specific themes of this session.
________________


1. Introduction: Utopian vs. Existential Claims
The Theme: Moving beyond headlines and social media hype.
   * Article Integration: Use Warzel (2025), "The world still hasn’t made sense of ChatGPT".
   * Application: When you discuss "maximalist treatment," quote Warzel’s description of the "rapture-to-bullshit continuum". This highlights how the public is caught between a promised technological revolution and an economic bubble.
   * Article Integration: Use Vara (2025), "ChatGPT’s Self-Serving Optimism".
   * Application: Use this to show that the "utopian promises" aren't just from social media—they are often hard-coded into the AI's "vibe" by companies like OpenAI to encourage growth and abundance narratives.
2. Context and College Education: Citizen, Instructor, Librarian
The Theme: Different roles having different perspectives.
   * Article Integration: Use Hassenfeld (2023), "How do AI systems like ChatGPT work?".
   * Application: In your "Librarian" section, emphasize the "Black Box" problem described by Sam Bowman. Librarians specialize in transparency; AI researchers often can't tell you how the model reached its conclusion, making the librarian's role as a guide to "information provenance" even more vital.
3. Purpose of College: Critical Literacy vs. Skill Training
The Theme: AI as a "calculator for writing" vs. an erosion of thinking.
   * Article Integration: Use Ceres (2023), "ChatGPT is coming for classrooms. Don’t panic." and McWhorter (2025), "My students use AI. So what?".
   * Application: These support the "Opportunity/Pedagogical" side of your tension. McWhorter argues that AI handles "busy work" (like grammar) so students can focus on arguments, while Ceres suggests educators should "critically appropriate" the tech.
   * Article Integration: Contrast with Rosario (2025), "I’m a high schooler. AI is demolishing my education.".
   * Application: This provides the "Existential Warning" from the user perspective—warning that outsourcing thought creates "eternal novices".
4. Machine Learning & Deep Learning: Complexity
The Theme: Moving from 2D linear models to 175-billion parameter LLMs.
   * Article Integration: Use Nield (2023), "How ChatGPT and other LLMs work".
   * Application: This reinforces your technical definitions. Nield explains the "transformer" architecture and the "self-attention mechanism" that allows these systems to calculate relationships between billions of signals.
5. AI as a "Prediction Machine": The "Dog/Cat/Pig/Horse" Example
The Theme: LLMs as statistical engines, not reasoning minds.
   * Article Integration: Use Newport (2023), "What kind of mind does ChatGPT have?".
   * Application: Newport’s deconstruction of "statistical text generation" perfectly mirrors your "Monkey at a typewriter" and "Dog/Cat" analogies. It grounds your claim that these are "automatons" without a model of the physical world.
   * Article Integration: Use Turk (2023), "How AI reduces the world to stereotypes".
   * Application: This is a perfect follow-up to the "Pig/Horse" prediction machine. When the training data is limited or biased, the "prediction" becomes a stereotype. For example, 99 out of 100 "Mexican person" prompts produced a sombrero because of "associative machine" logic in the training data.
6. Large Language Models (LLMs): The Blurst of Times
The Theme: Statistical discipline vs. human understanding.
   * Article Integration: Use Kriss (2025), "Why does A.I. write like … that?".
   * Application: Kriss explores the specific "rhetorical tells" that result from this statistical discipline. It’s not just "blurst," it’s an obsession with the "rule of threes" and the word "delve".
________________


Final Review Recommendation
Your content is very strong, especially the musical analogies. They make the "Learning Risk" tangible.
One minor suggestion: In the "Housing Price" section, you describe the simple model as "transparent." You might want to explicitly label the Deep Learning/LLM models as "Opaque" or "Black Box" using the Hassenfeld (2023) article to create a cleaner contrast for the students.
Would you like me to refine the "Baruch Policy" section to reflect how the student and faculty survey data (from Whitford or Flaherty) shows the tension between these different syllabus approaches?